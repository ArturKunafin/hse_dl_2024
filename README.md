# –í–≤–µ–¥–µ–Ω–∏–µ –≤ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ
# HSE SPRING 2024


- –ë–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫—É—Ä—Å–∞ –Ω–∞–≥–ª–æ –ø–æ–∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–∞ –∏–∑ –¥—Ä—É–≥–∏—Ö –∫—É—Ä—Å–æ–≤.
- –í —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∫ –∫–∞–∂–¥–æ–π –Ω–µ–¥–µ–ª–µ –±—É–¥—É—Ç –ø–æ—è–≤–ª—è—Ç—Å—è –º–∞—Ç–µ—Ä–∏–∞–ª—ã.
- –ú–∞—Ç–µ—Ä–∏–∞–ª—ã –ª–µ–∂–∞—Ç –≤ –ø–∞–ø–∫–∞—Ö `/week*`.
- –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–∫–∞—á–∞—Ç—å –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ø–∞–ø–∫—É, –ø—Ä–æ—Å—Ç–æ –≤—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ –Ω–µ—ë [–≤ —Å–µ—Ä–≤–∏—Å –¥–ª—è —Å–∫–∞—á–∫–∏.](https://minhaskamal.github.io/DownGit/#/home), –ª–∏–±–æ [—Ä–∞–∑–±–µ—Ä–∏—Ç–µ—Å—å —Å git](https://githowto.com/ru) –∏ —Å–¥–µ–ª–∞–π—Ç–µ pull. 

- üì¶ –í–∏–¥–µ–æ –∑–∞–ø–∏—Å–∏ —Å–µ–º–∏–Ω–∞—Ä–æ–≤ –∏ –ª–µ–∫—Ü–∏–π -> –±—É–¥—É—Ç –ø–æ—è–≤–ª—è—Ç—å—Å—è –≤ –±–µ—Å–µ–¥–µ –∫—É—Ä—Å–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–Ω—è—Ç–∏—è.
- üì¶ –õ—é–±—ã–µ –≤–æ–ø—Ä–æ—Å—ã –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –≤ —á–∞—Ç –∫—É—Ä—Å–∞, –≤—Å–µ –≤–∞–∂–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–∏—è –±—É–¥—É—Ç —Ç–∞–º –∂–µ.

## –ò–¥–µ–æ–ª–æ–≥–∏—è –∫—É—Ä—Å–∞ 
–ü–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ –Ω–∏–∫–∞–∫–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
–ù–µ–π—Ä–æ—Å–µ—Ç–∫–∞ ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –∞–Ω—Å–∞–º–±–ª—å –∏–∑ —Ä–µ–≥—Ä–µ—Å—Å–∏–π.

## –ü–ª–∞–Ω –∫—É—Ä—Å–∞

#### Feedforward NNs
01. - [MLP –Ω–µ –æ—Ç—Ö–æ–¥—è –æ—Ç –±—Ä–∞—É–∑–µ—Ä–∞](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.57027&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
02. - [–ë—ç–∫–ø—Ä–æ–ø –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ](http://www.habarov.spb.ru/lab_nnet/nn_js_lab/content/nn_js_backprop_2.html)
   - [–ü–æ—á–µ–º—É momentum —Ä–∞–±–æ—Ç–∞–µ—Ç](https://distill.pub/2017/momentum/)
   - [–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏](https://habr.com/ru/post/318970/)
   - [Bias correction in Adam](https://www.youtube.com/watch?v=-0ZMU-gnm2g)
   - [Cyclical Learning Rate](https://medium.com/swlh/cyclical-learning-rates-the-ultimate-guide-for-setting-learning-rates-for-neural-networks-3104e906f0ae)   
03. - [BatchNorm explained](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338)
   - [Custom weight decay](https://raberrytv.wordpress.com/2017/10/29/pytorch-weight-decay-made-easy/)
   - [Inverted dropout](https://www.coursera.org/lecture/deep-neural-network/dropout-regularization-eM33A)
   - [Weight initialization](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/)
#### CV
04. - [Convolution in CNN explained](https://www.youtube.com/watch?v=KTB_OFoAQcc)
   - [Dilated convolutions](https://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/)
05. - [ResNet](https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8)
   - [–ú–µ—Ç—Ä–∏–∫–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏](https://habr.com/ru/company/ods/blog/328372/)
   - [A Comprehensive Introduction to Different Types of Convolutions](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)
06. - [–ß—Ç–æ –≤—ã—É—á–∏–≤–∞–µ—Ç —Å–µ—Ç—å?](https://towardsdatascience.com/understanding-your-convolution-network-with-visualizations-a4883441533b)
   - [–ü–µ—Ä–µ–Ω–æ—Å —Å—Ç–∏–ª—è](https://towardsdatascience.com/neural-style-transfer-applications-data-augmentation-43d1dc1aeecc) 
   - [Denoising Autoencoders](https://medium.com/@garimanishad/reconstruct-corrupted-data-using-denoising-autoencoder-python-code-aeaff4b0958e)
   - [–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã](https://habr.com/ru/post/331382/)
07. - [StyleGAN3](https://nvlabs.github.io/stylegan3/)
   - [VAE ultimate guide & intuition](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)
   - [GAN explained](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans/lecture/gIAJ0/putting-it-all-together)
   
#### NLP
08. - [MLE explained](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)
   - [NLP guide –æ—Ç Lena Voita](https://lena-voita.github.io/nlp_course.html)
   - [Word2Vec tutorial —Å –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ–π –∏ –∫–æ–¥–æ–º](https://github.com/Yorko/mlcourse.ai/blob/main/jupyter_russian/tutorials/word2vec_demonzheg.ipynb)
   - [spaCy vs NLTK vs gensim](https://www.kaggle.com/faressayah/nlp-with-spacy-nltk-gensim)
09. - Stanford CS224N [all lectures](http://web.stanford.edu/class/cs224n/slides/). [Slides on RNN](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture07-fancy-rnn.pdf)
   - [NLTK book](https://www.nltk.org/book/)
   - [Deep RNNs](https://www.coursera.org/lecture/nlp-sequence-models/deep-rnns-ehs0S)
10. - [BERT, ELMO explained](https://jalammar.github.io/illustrated-bert/)
   - [Transformers explained](https://jalammar.github.io/illustrated-transformer/)
   - [Why BERT is called bidirectional?](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73)
   - [Harvard transformer implementation](http://nlp.seas.harvard.edu/2018/04/03/attention.html)


## –î–æ–º–∞—à–∫–∏ –∏ –ø—Ä–æ–µ–∫—Ç—ã

–°–ø–∏—Å–æ–∫ –¥–æ–º–∞—à–µ–∫: 

1. –û—Å–Ω–æ–≤—ã tensorflow –∏/–∏–ª–∏ pytorch, —Å–±–æ—Ä —Å–≤–æ–∏—Ö –ø–µ—Ä–≤—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –Ω–∞ tensorflow –∏/–∏–ª–∏ pytorch
2. –°–æ–±–∏—Ä–∞–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç—å —Å–≤–æ–∏–º–∏ —Ä—É–∫–∞–º–∏

### –ö–Ω–∏–≥–∏:

- üìö –ù–∏–∫–æ–ª–µ–Ω–∫–æ: ¬´–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –ü–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ –º–∏—Ä –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.¬ª ‚Äî –∫–Ω–∏–≥–∞, –∫–æ—Ç–æ—Ä—É—é –≤—Å–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É—é —á–∏—Ç–∞—Ç—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å.
- üìö –ì—É–¥—Ñ–µ–ª–ª–æ—É, –ë–µ–Ω–¥–∂–∏–æ: ¬´–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ.¬ª ‚Äî –±–∏–±–ª–∏—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –º–Ω–æ–≥–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∏ —Ö–æ—Ä–æ—à–µ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ–π.
- üìö –ü–µ–¥—Ä–æ –î–æ–º–∏–Ω–≥–æ—Å: ¬´–í–µ—Ä—Ö–æ–≤–Ω—ã–π –ê–ª–≥–æ—Ä–∏—Ç–º.¬ª ‚Äî –∫–Ω–∏–≥–∞ –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏ –º—ã—Å–ª—è—Ö –æ —Ç–æ–º, –∫–æ–≥–¥–∞ –∂–µ –Ω–∞—Å –ø–æ—Ä–∞–±–æ—Ç–∏—Ç –∏—Å–∫—É—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç.

### –û–Ω–ª–∞–π–Ω-–∫—É—Ä—Å—ã:

- ü§ñ [Advanced ML –æ—Ç –Ø–Ω–¥–µ–∫—Å–∞.](https://www.coursera.org/specializations/aml) –≠—Ç–æ –¥–ª—è —Ç–µ—Ö, –∫—Ç–æ —Ö–æ—á–µ—Ç —Ä–∞–∑–≤–∏–≤–∞—Ç—å—Å—è –¥–∞–ª—å—à–µ. –í —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –µ—Å—Ç—å —Ä–∞–∑–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∫—É—Ä—Å—ã. –ü–µ—Ä–≤—ã–π –∏–∑ –Ω–∏—Ö –ø—Ä–æ –Ω–µ–π—Ä–æ–Ω–∫–∏. –ö–æ–¥ –Ω–∞ Tensorflow. –í–µ—Ä—Å–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ç–∞–º –ø–æ–∫–∞ —á—Ç–æ —Å—Ç–∞—Ä–∞—è.
- ü§ñ [–ö—É—Ä—Å –Ω–µ–π—Ä–æ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞—é—Ç –≤ –®–ê–î –∏ —Å–∫–æ–ª—Ç–µ—Ö–µ.](https://github.com/yandexdataschool/Practical_DL/tree/master)  –ï—Å—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã –∫–æ–¥–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞—Ö. –ï—Å—Ç—å –≤–∏–¥–µ–æ –ª–µ–∫—Ü–∏–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º.
- ü§ñ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π [–∫—É—Ä—Å –ø–æ tf –æ—Ç Google.](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187) –ö–æ—Ä–æ—Ç–∫–∏–π. –ü–æ–∫—Ä—ã–≤–∞–µ—Ç –≤–µ—Å—å –±–∞–∑–æ–≤—ã–π Keras. –í—Å–µ —Ç–µ—Ç—Ä–∞–¥–∫–∏ –≤—ã–ª–æ–∂–µ–Ω—ã –≤ colab. –í–Ω—É—Ç—Ä–∏ –µ—Å—Ç—å –æ—á–µ–Ω—å —Å—Ç—Ä–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤—å—é.  
- ü§ñ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π [–∫—É—Ä—Å –ø–æ pytorch –æ—Ç Samsung.](https://stepik.org/course/50352/syllabus)  –°–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —á–∞—Å—Ç–µ–π: CV –∏ NLP. –î–ª—è —Ç–µ—Ö, –∫—Ç–æ —Ö–æ—á–µ—Ç –ø–∏—Å–∞—Ç—å –Ω–∞ pytorch.
- ü§ñ [Deep learning –Ω–∞ –ø–∞–ª—å—Ü–∞—Ö.](https://dlcourse.ai) –°–µ–º—ë–Ω –∏–∑ –∫—Ä–µ–º–Ω–∏–µ–≤–æ–π –¥–æ–ª–∏–Ω—ã –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ª–µ–∫—Ü–∏–∏ –Ω–∞ youtube. –û–±—ä—è—Å–Ω—è–µ—Ç –∏ –ø—Ä–∞–≤–¥–∞ –Ω–∞ –ø–∞–ª—å—Ü–∞—Ö. –ü—Ä–∏—á—ë–º —Å–ª–æ–∂–Ω—ã–µ –≤–µ—â–∏. –ö—Ä—É—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å –µ–≥–æ —Å—Ç—Ä–∏–º—ã –≤ –º–µ—Ç—Ä–æ, –ø–æ–∫–∞ –∫—É–¥–∞-—Ç–æ –µ–¥–µ—à—å. 
